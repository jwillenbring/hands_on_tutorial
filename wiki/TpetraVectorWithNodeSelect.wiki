#summary Create Tpetra Maps and (distributed) Vectors With Different Node Types

= Introduction =

See the [TpetraVector Tpetra Vector] example for a detailed description of this example.


= Code example =

The following example follows the same initialization steps as the
TpetraVector example, except we explicitly select a compute node type.

{{{
//
// Example: Creating distributed Tpetra vectors.
//

#include <Tpetra_DefaultPlatform.hpp>
#include <Tpetra_Vector.hpp>
#include <Tpetra_Version.hpp>
#include <Teuchos_GlobalMPISession.hpp>
#include <Teuchos_oblackholestream.hpp>
#include <Kokkos_TBBNode.hpp>

void
exampleRoutine (const Teuchos::RCP<const Teuchos::Comm<int> >& comm,
                std::ostream& out)
{
  using std::endl;
  using Teuchos::Array;
  using Teuchos::ArrayView;
  using Teuchos::RCP;
  using Teuchos::rcp;

  // Print out the Tpetra software version information.
  out << Tpetra::version() << endl << endl;
 
  // Set scalar, local ordinal and global ordinal types
  typedef double scalar_type;
  typedef int local_ordinal_type;
  typedef long global_ordinal_type;

  // The Kokkos "Node" type describes the type of shared-memory
  // parallelism that Tpetra will use _within_ an MPI process.  The
  // available Node types depend on Trilinos' build options and the
  // availability of certain third-party libraries.  Here are a few
  // examples:
  //
  // Kokkos::SerialNode: No parallelism
  //
  // Kokkos::TPINode: Uses a custom Pthreads wrapper
  //
  // Kokkos::TBBNode: Uses Intel's Threading Building Blocks
  //
  // Kokkos::ThrustNode: Uses Thrust, a C++ CUDA wrapper,
  //   for GPU parallelism.
  //
  Teuchos::ParameterList params;
  params.set<int>("Num Threads",4);
  params.set<int>("Verbose",true);
 
// Replace TBBNode in the following line with SerialNode, TPINode or ThrustNode as desired
  typedef Kokkos::TBBNode node_type;
 
 RCP<node_type> node = rcp(new node_type(params));

  //  Set map type
  typedef Tpetra::Map<local_ordinal_type, global_ordinal_type, node_type> map_type;


  //////////////////////////////////////////////////////////////////////
  // Create some Tpetra Map objects
  //////////////////////////////////////////////////////////////////////

   const Tpetra::global_size_t numGlobalElements = comm->getSize() * 5;
   const global_ordinal_type indexBase = 0;

  RCP<const map_type> contigMap = rcp (new map_type (numGlobalElements, indexBase, comm, Tpetra::GloballyDistributed, node));

  // contigMap is contiguous by construction.
  TEST_FOR_EXCEPTION(! contigMap->isContiguous(), std::logic_error,
                     "The supposedly contiguous Map isn't contiguous.");

   RCP<const map_type> cyclicMap;
  {
    Array<global_ordinal_type>::size_type numEltsPerProc = 5;
    Array<global_ordinal_type> elementList (numEltsPerProc);

    const int numProcs = comm->getSize();
    const int myRank = comm->getRank();
    for (Array<global_ordinal_type>::size_type k = 0; k < numEltsPerProc; ++k)
      elementList[k] = myRank + k*numProcs;
    
    cyclicMap = rcp (new map_type (numGlobalElements, elementList, indexBase, 
                                   comm, node));
  }

  // If there's more than one MPI process in the communicator,
  // then cyclicMap is definitely NOT contiguous.
  TEST_FOR_EXCEPTION(comm->getSize() > 1 && cyclicMap->isContiguous(),
                     std::logic_error, 
                     "The cyclic Map claims to be contiguous.");

  // contigMap and cyclicMap should always be compatible.  However, if
  // the communicator contains more than 1 process, then contigMap and
  // cyclicMap are NOT the same.
  TEST_FOR_EXCEPTION(! contigMap->isCompatible (*cyclicMap),
                     std::logic_error,
                     "contigMap should be compatible with cyclicMap, "
                     "but it's not.");
  TEST_FOR_EXCEPTION(comm->getSize() > 1 && contigMap->isSameAs (*cyclicMap),
                     std::logic_error,
                     "contigMap should be compatible with cyclicMap, "
                     "but it's not.");

  //////////////////////////////////////////////////////////////////////
  // We have maps now, so we can create vectors.
  //////////////////////////////////////////////////////////////////////
    typedef Tpetra::Vector<scalar_type, local_ordinal_type, 
    global_ordinal_type, node_type> vector_type;

  // Create a Vector with the contiguous Map.  This version of the
  // constructor will fill in the vector with zeros.
  RCP<vector_type> x = rcp (new vector_type (contigMap));

  // The copy constructor performs a deep copy.  
  // x and y have the same Map.
  RCP<vector_type> y = rcp (new vector_type (*x));

  // Create a Vector with the 1-D cyclic Map.  Calling the constructor
  // with false for the second argument leaves the data uninitialized,
  // so that you can fill it later without paying the cost of
  // initially filling it with zeros.
  RCP<vector_type> z = rcp (new vector_type (contigMap, false));

  // Set the entries of z to (pseudo)random numbers.  Please don't
  // consider this a good parallel pseudorandom number generator.
  z->randomize ();

  // Set the entries of x to all ones.

  // Using the ScalarTraits class ensures that the line of code below
  // will work for any valid Scalar type, even for complex numbers or
  // more exotic fields.
  x->putScalar (Teuchos::ScalarTraits<scalar_type>::one());

  const scalar_type alpha = 3.14159;
  const scalar_type beta = 2.71828;
  const scalar_type gamma = -10;

  // x = beta*x + alpha*z
  //
  // This is a legal operation!  Even though the Maps of x and z are
  // not the same, their Maps are compatible.
  x->update (alpha, *z, beta);

  y->putScalar (42);
  // y = gamma*y + alpha*x + beta*z
  y->update (alpha, *x, beta, *z, gamma);
  
  // Compute the 2-norm of y.  
  //
  // The norm may have a different type than Scalar.  For example, if
  // Scalar is complex, then the norm is real.  We can use the traits
  // class to get the type of the norm.
  typedef Teuchos::ScalarTraits<scalar_type>::magnitudeType magnitude_type;
  const magnitude_type theNorm = y->norm2 ();
  
  // Print the norm of y on Proc 0.
  out << "Norm of y: " << theNorm << endl;
}

//
// The same main() driver routine as in the TpetraInit example.
//
int 
main (int argc, char *argv[]) 
{
  using std::endl;
  using Teuchos::RCP;

  Teuchos::oblackholestream blackHole;
  Teuchos::GlobalMPISession mpiSession (&argc, &argv, &blackHole);
  RCP<const Teuchos::Comm<int> > comm = 
    Tpetra::DefaultPlatform::getDefaultPlatform().getComm();

  const int myRank = comm->getRank();
  std::ostream& out = (myRank == 0) ? std::cout : blackHole;

  // We have a communicator and an output stream.
  // Let's do something with them!
  exampleRoutine (comm, out);

  return 0;
}
}}}