#summary Tpetra's distributed vectors and data distribution objects

= Lesson topics =

In this lesson, we will explain how to create the simplest kind of Tpetra linear algebra object: a Vector, whose entries are distributed over the process(es) in a communicator.  The Map object describes this distribution of entries over processes.  You create the Map to describe the distribution scheme you want, and then use the Map to create objects (such as Vectors) that have this distribution.  We give examples of different distributions you can create, use their Maps to create Vectors, and then do some arithmetic with the Vectors.  All of this gives us an opportunity to explain the various template parameters that are part of the type of nearly every Tpetra object.

= A Map describes a data distribution =

[http://trilinos.sandia.gov/packages/tpetra/ Tpetra], like
[http://trilinos.sandia.gov/packages/epetra/ Epetra], uses objects
called "maps" to encapsulate the details of distributing data over MPI
processes.  Maps make data distribution into a first-class citizen.
You can think of them abstractly as representing a vector space.  If
two vectors have the same map, it's like they come from the same
vector space.  For example, you can add them together without
performing communication.  If they come from different vector spaces,
then you can't add the vectors together unless Tpetra knows that their
vector spaces are [https://secure.wikimedia.org/wikipedia/en/wiki/Isomorphism isomorphic] (in Tpetra language, that "their Maps are compatible").

You can find documentation for Tpetra's Map class [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Map.html here].

== Map compatibility ==

Tpetra gives two ways to compare two Maps.  Two Maps may either be
"compatible" (`map1.isCompatible(map2)`) or "the same"
(`map1.isSameAs(map2)`).  Compatibility of two Maps corresponds to
isomorphism of two vector spaces.  Two Maps that are the same are
always compatible.  The `isCompatible()` criterion is less
restrictive, and also less expensive to check (although checking for
compatibility requires a reduction on a Boolean over all processes in
the Map's communicator).  

Adding together two vectors with compatible but not the same Maps is legal.  It might not make mathematical sense, depending on your application, because entries of the vectors are ordered differently.  Adding together two vectors with the same Maps is both legal, and always mathematically correct.

Both sameness and compatibility are commutative Boolean relations:  for example, `map1.isCompatible(map2)` means `map2.isCompatible(map1)`.

`Map::isSameAs()` specifically means:
  * the min and max global indices are the same
  * the global number of elements is the same
  * the maps are both distributed, or both not distributed
  * the maps have the same index base
  * MPI processes in the Map's communicator that have the same MPI rank, own the same number of elements.
  * MPI processes in the Map's communicator that have the same MPI rank, own the same elements.  That is, their elements have the same IDs.
  
`Map::isCompatible()` specifically means:
  * the global number of elements is the same
  * MPI processes in the Map's communicator that have the same MPI rank, own the same number of elements.

== A Map assigns elements of a data structure to processes ==

For you as the user, the fact that you might be parallelizing your application using MPI is really an implementation detail.  You care about what we call _global indices._  These represent the elements of a distributed object (such as rows or columns of a sparse matrix, or elements of a vector) uniquely over the entire object.  The object in turn may be distributed over multiple processes.  Just about any data structure containing elements that can be assigned an integer index can be distributed using a Map.  

A Map assigns global indices to parallel processes.  For efficiency, within a process, we refer to a global index using its "local index."  _Local indices_ are local to the process that owns them. If global index G is owned by process P, then there is a unique local index L on process P corresponding to G. If the local index L is valid on process P, then there is a unique global index G owned by P corresponding to the pair (L, P). However, multiple processes might own the same global index (an "overlapping Map"), so a global index G might correspond to multiple (L, P) pairs. In summary, local indices on a process correspond to object elements (e.g., sparse matrix rows or columns) owned by that process.

== You get to specify the types of local and global indices ==

In Tpetra, the types of local and global indices are template parameters of Map, Vector, CrsMatrix, and other distributed objects.  Local indices have type !LocalOrdinal, and global indices have type !GlobalOrdinal.  Both should be signed built-in C++ integer types.  However, you get to pick their size, based on how big your problem is.  If your problem has more than 2 billion elements, you will need a 64-bit integer type (such as `long long` or `int64_t`) for !GlobalOrdinal, but if you have enough processes so that no one process stores more than 2 billion elements locally, then you may use a 32-bit integer type (such as `int` typically, or `int32_t`) for !LocalOrdinal.  It is usually more efficient to use the shortest integer type possible.  The default type of both !LocalOrdinal and !GlobalOrdinal is `int`.

Tpetra differs from Epetra in that you, the user, get to decide the types of local and global indices. In Epetra, local and global indices both used to have type int. With the latest Trilinos release, Epetra uses 64-bit integers for global indices, and 32-bit integers (`int`) for local indices.  Tpetra lets you decide the types of each.

== Other differences between Tpetra and Epetra ==

Tpetra's maps look different than Epetra's maps because of all the
template parameters, but they work similiarly.  One difference is that
Tpetra maps tend to be handled by RCP (reference-counted smart
pointer) rather than copied or passed by const reference.  Another
difference is that !Epetra_Map inherits from !Epetra_BlockMap, whereas
in Tpetra, Map and !BlockMap do not have an inheritance relationship.
!Epetra_Map only has a `SameAs()` predicate, whereas Tpetra's
Map class distinguishes between "compatibility" and "sameness" (see
above).  Finally, !Epetra_Map's `SameAs()` means about the same thing as Tpetra's `isSameAs()`.

= Tpetra::Vector =

You'll find documentation for Tpetra's Vector class
[http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Vector.html here].  Vector inherits from Tpetra's [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1MultiVector.html MultiVector] class, which represents a collection of one or more vectors with the same Map.  Tpetra favors block algorithms, so it favors !MultiVectors over single Vectors.  A single Vector is just a !MultiVector containing one vector, with a few convenience methods.

Vector's interface contains some common linear algebra operations for vector-vector operations (including those one would find in the BLAS 1 standard).  You are encouraged to try out the new `Tpetra::RTI` (Reduce Transform Interface) for implementing more complicated operations on Vectors.  (RTI is kind of like the C++ Standard Template Library's iterators, except for distributed objects and for parallel iteration.)

= Tpetra objects' template parameters =

Most Tpetra objects, including Map and Vector, take several different template parameters.  Some of them have default values.  For example, Vector has the following template parameters:
  * Scalar: The type of data stored in the vector
  * !LocalOrdinal: The integer type of local indices
  * !GlobalOrdinal: The integer type of global indices
  * Node: The implementation of intranode (within a node) parallelism

Map has the same template parameters, except for Scalar (since the same Map can be used to describe Vectors with different Scalar types).  

= Code example =

The following example follows the same initialization steps as the
TpetraInit example.  It then creates two distributed Tpetra maps and
some Tpetra vectors, and does a few computations with the vectors.

{{{
//
// Example: Creating distributed Tpetra vectors.
//

#include <Tpetra_DefaultPlatform.hpp>
#include <Tpetra_Vector.hpp>
#include <Tpetra_Version.hpp>
#include <Teuchos_GlobalMPISession.hpp>
#include <Teuchos_oblackholestream.hpp>

void
exampleRoutine (const Teuchos::RCP<const Teuchos::Comm<int> >& comm,
		std::ostream& out)
{
  using std::endl;
  using Teuchos::Array;
  using Teuchos::ArrayView;
  using Teuchos::RCP;
  using Teuchos::rcp;

  // Print out the Tpetra software version information.
  out << Tpetra::version() << endl << endl;

  //
  // The first thing you might notice that makes Tpetra objects
  // different than their Epetra counterparts, is that Tpetra objects
  // take several template parameters.  These template parameters give
  // Tpetra its features of being able to solve very large problems
  // (of more than 2 billion unknowns) and to exploit intranode
  // parallelism.
  //
  // It's common to begin a Tpetra application with some typedefs to
  // make the code more concise and readable.  They also make the code
  // more maintainable, since you can change the typedefs without
  // changing the rest of the program.
  //

  // The "Scalar" type is the type of the values stored in the Tpetra
  // objects.  Valid Scalar types include real or complex
  // (std::complex<T>) floating-point types, or more exotic objects
  // with similar behavior.
  typedef double scalar_type;

  // The "LocalOrdinal" (LO) type is the type of "local" indices.
  // Both Epetra and Tpetra index local elements differently than
  // global elements.  Tpetra exploits this so that you can use a
  // shorter integer type for local indices.  This saves bandwidth
  // when computing sparse matrix-vector products.
  typedef int local_ordinal_type;

  // The "GlobalOrdinal" (GO) type is the type of "global" indices.
  typedef long global_ordinal_type;

  // The Kokkos "Node" type describes the type of shared-memory
  // parallelism that Tpetra will use _within_ an MPI process.  The
  // available Node types depend on Trilinos' build options and the
  // availability of certain third-party libraries.  Here are a few
  // examples:
  //
  // Kokkos::SerialNode: No parallelism
  //
  // Kokkos::TPINode: Uses a custom Pthreads wrapper
  //
  // Kokkos::TBBNode: Uses Intel's Threading Building Blocks
  //
  // Kokkos::ThrustNode: Uses Thrust, a C++ CUDA wrapper,
  //   for GPU parallelism.
  //
  // Using a GPU-oriented Node means that Tpetra objects that store a
  // lot of data (vectors and sparse matrices, for example) will store
  // that data on the GPU, and operate on it there whenever possible.
  //
  // Kokkos::DefaultNode gives you a default Node type.  It may be
  // different, depending on Trilinos' build options.  Currently, for
  // example, building Trilinos with Pthreads enabled gives you
  // Kokkos::TPINode by default.  That means your default Node is a
  // parallel node!
  typedef Kokkos::DefaultNode::DefaultNodeType node_type;

  // Maps know how to convert between local and global indices, so of
  // course they are templated on the local and global Ordinal types.
  // They are also templated on the Kokkos Node type, because Tpetra
  // objects that use Tpetra::Map are.  It's important not to mix up
  // Maps for different Kokkos Node types.
  typedef Tpetra::Map<local_ordinal_type, global_ordinal_type, node_type> map_type;

  // Get a pointer to the default Kokkos Node.  We'll need this when
  // creating the Tpetra::Map objects.  
  //
  // Currently, if you want a node of a different type, you have to
  // instantiate it explicitly.  We are moving to a model whereby you
  // template your entire program on the Node type, and the system
  // gives you the appropriate Node instantiation.
  RCP<node_type> node = Kokkos::DefaultNode::getDefaultNode ();

  //////////////////////////////////////////////////////////////////////
  // Create some Tpetra Map objects
  //////////////////////////////////////////////////////////////////////

  //
  // Like Epetra, Tpetra has local and global Maps.  Local maps
  // describe objects that are replicated over all participating MPI
  // processes.  Global maps describe distributed objects.  You can do
  // imports and exports between local and global maps; this is how
  // you would turn locally replicated objects into distributed
  // objects and vice versa.
  //

  // The total (global, i.e., over all MPI processes) number of
  // elements in the Map.  Tpetra's global_size_t type is an unsigned
  // type and is at least 64 bits long on 64-bit machines.
  //
  // For this example, we scale the global number of elements in the
  // Map with the number of MPI processes.  That way, you can run this
  // example with any number of MPI processes and every process will
  // still have a positive number of elements.
  const Tpetra::global_size_t numGlobalElements = comm->getSize() * 5;

  // Tpetra can index the elements of a Map starting with 0 (C style),
  // 1 (Fortran style), or any base you want.  1-based indexing is
  // handy when interfacing with Fortran.  We choose 0-based indexing
  // here.
  const global_ordinal_type indexBase = 0;

  // Construct a Map that puts the same number of equations on each
  // processor.  Pass in the Kokkos Node, so that this line of code
  // will work with any Kokkos Node type.
  //
  // It's typical to create a const Map.  Maps should be considered
  // immutable objects.  If you want a new data distribution, create a
  // new Map.
  RCP<const map_type> contigMap = 
    rcp (new map_type (numGlobalElements, indexBase, comm, 
		       Tpetra::GloballyDistributed, node));

  // contigMap is contiguous by construction.
  TEST_FOR_EXCEPTION(! contigMap->isContiguous(), std::logic_error,
		     "The supposedly contiguous Map isn't contiguous.");

  // Let's create a second Map.  It will have the same number of
  // global elements per process, but will distribute them
  // differently, in round-robin (1-D cyclic) fashion instead of
  // contiguously.
  RCP<const map_type> cyclicMap;
  {
    // We'll use the version of the Map constructor that takes, on
    // each MPI process, a list of the global elements in the Map
    // belonging to that process.  You can use this constructor to
    // construct an overlapping (also called "not 1-to-1") Map, in
    // which one or more elements are owned by multiple processes.  We
    // don't do that here; we make a nonoverlapping (also called
    // "1-to-1") Map.
    Array<global_ordinal_type>::size_type numEltsPerProc = 5;
    Array<global_ordinal_type> elementList (numEltsPerProc);

    const int numProcs = comm->getSize();
    const int myRank = comm->getRank();
    for (Array<global_ordinal_type>::size_type k = 0; k < numEltsPerProc; ++k)
      elementList[k] = myRank + k*numProcs;
    
    cyclicMap = rcp (new map_type (numGlobalElements, elementList, indexBase, 
				   comm, node));
  }

  // If there's more than one MPI process in the communicator,
  // then cyclicMap is definitely NOT contiguous.
  TEST_FOR_EXCEPTION(comm->getSize() > 1 && cyclicMap->isContiguous(),
		     std::logic_error, 
		     "The cyclic Map claims to be contiguous.");

  // contigMap and cyclicMap should always be compatible.  However, if
  // the communicator contains more than 1 process, then contigMap and
  // cyclicMap are NOT the same.
  TEST_FOR_EXCEPTION(! contigMap->isCompatible (*cyclicMap),
		     std::logic_error,
		     "contigMap should be compatible with cyclicMap, "
		     "but it's not.");
  TEST_FOR_EXCEPTION(comm->getSize() > 1 && contigMap->isSameAs (*cyclicMap),
		     std::logic_error,
		     "contigMap should be compatible with cyclicMap, "
		     "but it's not.");

  //////////////////////////////////////////////////////////////////////
  // We have maps now, so we can create vectors.
  //////////////////////////////////////////////////////////////////////

  // Since Tpetra::Vector takes four template parameters, its type is
  // long.  It's helpful to use a typedef.  C++0x's type inference
  // feature is also useful.
  typedef Tpetra::Vector<scalar_type, local_ordinal_type, 
    global_ordinal_type, node_type> vector_type;

  // Create a Vector with the contiguous Map.  This version of the
  // constructor will fill in the vector with zeros.
  RCP<vector_type> x = rcp (new vector_type (contigMap));

  // The copy constructor performs a deep copy.  
  // x and y have the same Map.
  RCP<vector_type> y = rcp (new vector_type (*x));

  // Create a Vector with the 1-D cyclic Map.  Calling the constructor
  // with false for the second argument leaves the data uninitialized,
  // so that you can fill it later without paying the cost of
  // initially filling it with zeros.
  RCP<vector_type> z = rcp (new vector_type (contigMap, false));

  // Set the entries of z to (pseudo)random numbers.  Please don't
  // consider this a good parallel pseudorandom number generator.
  z->randomize ();

  // Set the entries of x to all ones.

  // Using the ScalarTraits class ensures that the line of code below
  // will work for any valid Scalar type, even for complex numbers or
  // more exotic fields.
  x->putScalar (Teuchos::ScalarTraits<scalar_type>::one());

  const scalar_type alpha = 3.14159;
  const scalar_type beta = 2.71828;
  const scalar_type gamma = -10;

  // x = beta*x + alpha*z
  //
  // This is a legal operation!  Even though the Maps of x and z are
  // not the same, their Maps are compatible.
  x->update (alpha, *z, beta);

  y->putScalar (42);
  // y = gamma*y + alpha*x + beta*z
  y->update (alpha, *x, beta, *z, gamma);
  
  // Compute the 2-norm of y.  
  //
  // The norm may have a different type than Scalar.  For example, if
  // Scalar is complex, then the norm is real.  We can use the traits
  // class to get the type of the norm.
  typedef Teuchos::ScalarTraits<scalar_type>::magnitudeType magnitude_type;
  const magnitude_type theNorm = y->norm2 ();
  
  // Print the norm of y on Proc 0.
  out << "Norm of y: " << theNorm << endl;
}

//
// The same main() driver routine as in the TpetraInit example.
//
int 
main (int argc, char *argv[]) 
{
  using std::endl;
  using Teuchos::RCP;

  Teuchos::oblackholestream blackHole;
  Teuchos::GlobalMPISession mpiSession (&argc, &argv, &blackHole);
  RCP<const Teuchos::Comm<int> > comm = 
    Tpetra::DefaultPlatform::getDefaultPlatform().getComm();

  const int myRank = comm->getRank();
  std::ostream& out = (myRank == 0) ? std::cout : blackHole;

  // We have a communicator and an output stream.
  // Let's do something with them!
  exampleRoutine (comm, out);

  return 0;
}
}}}