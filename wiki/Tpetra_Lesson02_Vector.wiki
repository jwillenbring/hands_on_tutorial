#summary Tpetra Lesson 02: Map and Vector (distributions and distributed objects)

= Lesson topics =

In this lesson, we will explain how to create the simplest kind of Tpetra linear algebra object: a Vector, whose entries are distributed over the process(es) in a communicator.  The [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Map.html Map] object describes this distribution of entries over processes.  You create a Map to describe the distribution scheme you want, and then use the Map to create objects (such as Vectors) that have this distribution.  We spend a little bit more time than you might initially wish explaining Map, but understanding it is important for getting the best performance out of Teptra.  We give examples of different distributions you can create, use their Maps to create Vectors, and then do some arithmetic with the Vectors.  All of this gives us an opportunity to explain the various template parameters that are part of the type of nearly every Tpetra object.

= `Tpetra::Map` =

== A Map instance describes a data distribution ==

[http://trilinos.sandia.gov/packages/tpetra/ Tpetra], like
[http://trilinos.sandia.gov/packages/epetra/ Epetra], uses objects
called "Maps" to encapsulate the details of distributing data over MPI
processes.  Maps make data distribution into a first-class citizen.
Each Map instance represents a particular data distribution.

You can think of a Map instance abstractly as representing a vector space.  
If two vectors have the same map, it's like they come from the same
vector space.  For example, you can add them together without
performing communication.  If they come from different vector spaces,
then you need more information to know whether it is legal to add the 
vectors together.

You can find documentation for Tpetra's Map class [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Map.html here].

== A Map assigns elements of a data structure to processes ==

=== Global indices matter to you ===

For you as the user, the fact that you might be parallelizing your application using MPI is really an implementation detail.  You care about what we call _global indices._  These represent the elements of a distributed object (such as rows or columns of a sparse matrix, or elements of a vector) uniquely over the entire object.  The object in turn may be distributed over multiple processes.  Just about any data structure containing elements that can be assigned an integer index can be distributed using a Map.  For most Tpetra users, this means elements of a vector, rows of a !MultiVector, or rows or columns of a sparse matrix.  However, it is not limited to these kinds of objects.  You may even use Map for your own distributed objects.

A Map assigns global indices to parallel processes.  If it assigns a global index G to a process P, we say that process P _owns_ global index G.  It is legal for multiple processes to own the same global index G.  In fact, this is how we implement many useful communication patterns, including those in sparse matrix-vector multiply.  We won't go into much detail in this lesson about that.

=== Local indices are an implementation detail ===

For efficiency, within a process, we refer to a global index using its "local index" on that process.  _Local indices_ are local to the process that owns them. If process P owns global index G, then there is a unique local index L on process P corresponding to G. If the local index L is valid on process P, then there is a unique global index G owned by P corresponding to the pair (L, P). However, multiple processes might own the same global index, so a global index G might correspond to multiple (L, P) pairs. In summary, local indices on a process correspond to object elements (e.g., sparse matrix rows or columns) owned by that process.

=== We expose local indices for performance reasons ===

Local indices matter to you because it may be more efficient to use them to access or modify local data than it is to use global indices.  This is because distributed data structures must convert from global to local indices every time a user asks for an element by its global index.  This requires a table lookup in general, since a process may own an arbitrary subset of all the global indices, in an arbitrary order.  Even though local indices are an implementation detail, we expose them because avoiding that table lookup on each access can improve performance a lot.

=== Maps are themselves distributed data ===

If a Map has N global elements over P processes, and if no one process owns all the global elements, we _never_ store all N global indices on a single process.  Some kinds of Maps require storing all the global indices, but in this case, the indices are themselves distributed over processes.  This ensures _memory scalability_ (no one process has to store all the data).

== Map compatibility ==

We mentioned above that a Map behaves much like a vector space.  For instance, if two Vectors have the same Map, it is both legal and meaningful to add them together.  This makes it useful to be able to compare Maps.  Tpetra gives two ways to compare two Maps.  Two Maps may either be "compatible" (`map1.isCompatible(map2)`) or "the same" (`map1.isSameAs(map2)`).  

Compatibility of two Maps corresponds to [https://secure.wikimedia.org/wikipedia/en/wiki/Isomorphism isomorphism] of two vector spaces.  Two Maps that are the same are always compatible.  The `isCompatible()` criterion is less restrictive, and also less expensive to check (although checking for compatibility requires a reduction on a Boolean over all processes in the Map's communicator).  

Adding together two vectors with compatible but not the same Maps is legal.  It might not make mathematical sense, depending on your application.  This is because entries of the vectors are ordered differently.  (Also, just because two vector spaces are isomorphic, doesn't necessarily mean that adding elements of one to elements of another makes sense.)  Adding together two vectors with the same Maps is both legal and mathematically sensible.

Both sameness and compatibility are commutative Boolean relations:  for example, `map1.isCompatible(map2)` means `map2.isCompatible(map1)`.

Two Maps are _compatible_ when:
  * they have the same global number of elements
  * MPI processes in the Map's communicator that have the same MPI rank, own the same number of elements.

Two Maps are _the same_ when:
  * their minimum and maximum global indices are the same
  * they have the same global number of elements
  * the Maps are both distributed over multiple processes, or both not distributed over multiple processes
  * the Maps have the same _index base_ (this means the smallest legal global index value, more or less)
  * Processes that have the same rank, own the same number of elements.
  * Processes that have the same rank, own the same elements.  That is, their elements have the same indices, in the same order.

== You get to specify the types of local and global indices ==

In Tpetra, the types of local and global indices are template parameters of Map, Vector, !CrsMatrix, and other distributed objects.  Local indices have type !LocalOrdinal, and global indices have type !GlobalOrdinal.  Both should be signed built-in C++ integer types.  However, you get to pick their size, based on how big your problem is.  If your problem has more than 2 billion elements, you will need a 64-bit integer type (such as `long long` or `int64_t`) for !GlobalOrdinal, but if you have enough processes so that no one process stores more than 2 billion elements locally, then you may use a 32-bit integer type (such as `int` typically, or `int32_t`) for !LocalOrdinal.  The default type of both !LocalOrdinal and !GlobalOrdinal is `int`.

It is usually more efficient to use the shortest integer type possible for both local and global indices.  "Shortest" means fewest number of bits.  Fewer bits mean you use less memory and thus can solve bigger problems or use higher-quality preconditioners that solve problems in fewer iterations.  Shorter local indices can also mean better performance for local sparse matrix kernels, such as sparse matrix-vector multiply, sparse triangular solve, and smoothing (for algebraic multigrid).

Tpetra differs from Epetra in that you, the user, get to decide the types of local and global indices. In Epetra, local and global indices both used to have type int. With the latest Trilinos release, Epetra uses 64-bit integers for global indices, and 32-bit integers (`int`) for local indices.  Tpetra lets you decide the types of each.  

== Different categories of Maps ==

=== One to one ===

A Map is _one to one_ if each global index in the Map is owned by only one process.  This means that the function from global index G to its local index and process rank (L,P) is one to one in a mathematical sense ("injective").  In this case, the function is only onto ("surjective") if there is only one process.  Knowing whether a Map is one-to-one is important for data redistribution, which Tpetra exposes as the [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Import.html Import] and [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Export.html Export] operations.  We will cover Import and Export in subsequent lessons.

An example of a one-to-one Map is a Map containing 101 global indices 0 .. 100 and distributed over four processes, where 
  - Process 0 owns 0 .. 24
  - Process 1 owns 25 .. 49
  - Process 2 owns 50 .. 74
  - Process 3 owns 75 .. 100

An example of a _not_ one-to-one Map is a Map containing 101 global indices 0 .. 100 and distributed over four processes, where 
  - Process 0 owns 0 .. 25
  - Process 1 owns 25 .. 50
  - Process 2 owns 50 .. 75
  - Process 3 owns 75 .. 100

Note the overlap of one global index between each "adjacent" process.  An example of a mathematical problem with an overlapping distribution like this would be a 1-D linear finite element or finite difference discretization, where elements are distributed with unique ownership among the processes, but the boundary node between two adjacent elements on different processes is shared among those two processes.

=== Contiguous or noncontiguous, uniform or not ===

A Map is _contiguous_ when each process' list of global indices forms an interval and is strictly increasing, and the globally minimum global index equals the index base. Map optimizes for the contiguous case. In particular, noncontiguous Maps require communication in order to figure out which process owns a particular global index.

Note that in Tpetra, "contiguous" is an optimization, not a predicate.  Tpetra may not necessarily work hard to check contiguity.  The best way to ensure that your Map is contiguous is to use one of the two constructors that always make a contiguous Map.

An example of a contiguous Map is one containing 101 global indices 0 .. 100 and distributed over four processes, where 
  - Process 0 owns 0 .. 24
  - Process 1 owns 25 .. 49
  - Process 2 owns 50 .. 74
  - Process 3 owns 75 .. 100

Note that Process 3 in this example owns 26 global indices, whereas the other processes each own 25.  We say that a Map is _uniform_ if each process owns the same number of global indices.  The above Map is _not_ uniform.  Map includes both a constructor for uniform contiguous Maps, where you specify the total number of global indices, and a constructor for possibly nonuniform contiguous Maps, where you specify the number of global indices owned by each process.

=== Globally distributed or locally replicated ===

_Globally distributed_ means that all of the following are true:
  # The Map's communicator has more than one process. 
  # There is at least one process in the Map's communicator, whose local number of elements does not equal the number of global elements. (That is, not all the elements are replicated over all the processes.)

If at least one of the above are not true, then we call the Map _locally replicated_.  The two terms are mutually exclusive. 

== Other differences between Tpetra and Epetra ==

Tpetra's maps look different than Epetra's maps because of all the
template parameters, but they work similiarly.  One difference is that
Tpetra maps tend to be handled by RCP (reference-counted smart
pointer) rather than copied or passed by const reference.  Another
difference is that !Epetra_Map inherits from !Epetra_BlockMap, whereas
in Tpetra, Map and !BlockMap do not have an inheritance relationship.
!Epetra_Map only has a `SameAs()` predicate, whereas Tpetra's
Map class distinguishes between "compatibility" and "sameness" (see
above).  Finally, !Epetra_Map's `SameAs()` means about the same thing as Tpetra's `isSameAs()`.

= `Tpetra::Vector` =

`Tpetra::Vector` implements a finite-dimensional vector distributed over processes.  Vector inherits from Tpetra's [http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1MultiVector.html MultiVector] class, which represents a collection of one or more vectors with the same Map.  Tpetra favors block algorithms, so it favors !MultiVectors over single Vectors.  A single Vector is just a !MultiVector containing one vector, with a few convenience methods.  You'll find documentation for Tpetra's Vector class
[http://trilinos.sandia.gov/packages/docs/dev/packages/tpetra/doc/html/classTpetra_1_1Vector.html here].  

Vector's interface contains some common linear algebra operations for vector-vector operations, including operations analogous to those in the BLAS 1 standard.  You are encouraged to try out the new `Tpetra::RTI` (Reduce Transform Interface) for implementing more complicated operations on Vectors.  (RTI is kind of like the C++ Standard Template Library's iterators, except for distributed objects and for parallel iteration.)

= Tpetra objects' template parameters =

Most Tpetra objects, including Map and Vector, take several different template parameters.  Some of them have default values.  For example, Vector has the following template parameters:
  * Scalar: The type of data stored in the vector
  * !LocalOrdinal: The integer type of local indices
  * !GlobalOrdinal: The integer type of global indices
  * Node: The implementation of intranode (within a node) parallelism

Map has the same template parameters, except for Scalar (since the same Map can be used to describe Vectors with different Scalar types).  

= Code example =

The following example follows the same initialization steps as in the previous lesson.  It then creates two distributed Tpetra Maps and some Tpetra Vectors, and does a few computations with the vectors.

{{{
//
// Example: Creating distributed Tpetra vectors.
//

#include <Tpetra_DefaultPlatform.hpp>
#include <Tpetra_Vector.hpp>
#include <Tpetra_Version.hpp>
#include <Teuchos_GlobalMPISession.hpp>
#include <Teuchos_oblackholestream.hpp>

void
exampleRoutine (const Teuchos::RCP<const Teuchos::Comm<int> >& comm,
		std::ostream& out)
{
  using std::endl;
  using Teuchos::Array;
  using Teuchos::ArrayView;
  using Teuchos::RCP;
  using Teuchos::rcp;

  // Print out the Tpetra software version information.
  out << Tpetra::version() << endl << endl;

  //
  // The first thing you might notice that makes Tpetra objects
  // different than their Epetra counterparts, is that Tpetra objects
  // take several template parameters.  These template parameters give
  // Tpetra its features of being able to solve very large problems
  // (of more than 2 billion unknowns) and to exploit intranode
  // parallelism.
  //
  // It's common to begin a Tpetra application with some typedefs to
  // make the code more concise and readable.  They also make the code
  // more maintainable, since you can change the typedefs without
  // changing the rest of the program.
  //

  // The "Scalar" type is the type of the values stored in the Tpetra
  // objects.  Valid Scalar types include real or complex
  // (std::complex<T>) floating-point types, or more exotic objects
  // with similar behavior.
  typedef double scalar_type;

  // The "LocalOrdinal" (LO) type is the type of "local" indices.
  // Both Epetra and Tpetra index local elements differently than
  // global elements.  Tpetra exploits this so that you can use a
  // shorter integer type for local indices.  This saves bandwidth
  // when computing sparse matrix-vector products.
  typedef int local_ordinal_type;

  // The "GlobalOrdinal" (GO) type is the type of "global" indices.
  typedef long global_ordinal_type;

  // The Kokkos "Node" type describes the type of shared-memory
  // parallelism that Tpetra will use _within_ an MPI process.  The
  // available Node types depend on Trilinos' build options and the
  // availability of certain third-party libraries.  Here are a few
  // examples:
  //
  // Kokkos::SerialNode: No parallelism
  //
  // Kokkos::TPINode: Uses a custom Pthreads wrapper
  //
  // Kokkos::TBBNode: Uses Intel's Threading Building Blocks
  //
  // Kokkos::ThrustNode: Uses Thrust, a C++ CUDA wrapper,
  //   for GPU parallelism.
  //
  // Using a GPU-oriented Node means that Tpetra objects that store a
  // lot of data (vectors and sparse matrices, for example) will store
  // that data on the GPU, and operate on it there whenever possible.
  //
  // Kokkos::DefaultNode gives you a default Node type.  It may be
  // different, depending on Trilinos' build options.  Currently, for
  // example, building Trilinos with Pthreads enabled gives you
  // Kokkos::TPINode by default.  That means your default Node is a
  // parallel node!
  typedef Kokkos::DefaultNode::DefaultNodeType node_type;

  // Maps know how to convert between local and global indices, so of
  // course they are templated on the local and global Ordinal types.
  // They are also templated on the Kokkos Node type, because Tpetra
  // objects that use Tpetra::Map are.  It's important not to mix up
  // Maps for different Kokkos Node types.
  typedef Tpetra::Map<local_ordinal_type, global_ordinal_type, node_type> map_type;

  // Get a pointer to the default Kokkos Node.  We'll need this when
  // creating the Tpetra::Map objects.  
  //
  // Currently, if you want a node of a different type, you have to
  // instantiate it explicitly.  We are moving to a model whereby you
  // template your entire program on the Node type, and the system
  // gives you the appropriate Node instantiation.
  RCP<node_type> node = Kokkos::DefaultNode::getDefaultNode ();

  //////////////////////////////////////////////////////////////////////
  // Create some Tpetra Map objects
  //////////////////////////////////////////////////////////////////////

  //
  // Like Epetra, Tpetra has local and global Maps.  Local maps
  // describe objects that are replicated over all participating MPI
  // processes.  Global maps describe distributed objects.  You can do
  // imports and exports between local and global maps; this is how
  // you would turn locally replicated objects into distributed
  // objects and vice versa.
  //

  // The total (global, i.e., over all MPI processes) number of
  // elements in the Map.  Tpetra's global_size_t type is an unsigned
  // type and is at least 64 bits long on 64-bit machines.
  //
  // For this example, we scale the global number of elements in the
  // Map with the number of MPI processes.  That way, you can run this
  // example with any number of MPI processes and every process will
  // still have a positive number of elements.
  const Tpetra::global_size_t numGlobalElements = comm->getSize() * 5;

  // Tpetra can index the elements of a Map starting with 0 (C style),
  // 1 (Fortran style), or any base you want.  1-based indexing is
  // handy when interfacing with Fortran.  We choose 0-based indexing
  // here.
  const global_ordinal_type indexBase = 0;

  // Construct a Map that puts the same number of equations on each
  // processor.  Pass in the Kokkos Node, so that this line of code
  // will work with any Kokkos Node type.
  //
  // It's typical to create a const Map.  Maps should be considered
  // immutable objects.  If you want a new data distribution, create a
  // new Map.
  RCP<const map_type> contigMap = 
    rcp (new map_type (numGlobalElements, indexBase, comm, 
		       Tpetra::GloballyDistributed, node));

  // contigMap is contiguous by construction.
  TEUCHOS_TEST_FOR_EXCEPTION(! contigMap->isContiguous(), std::logic_error,
    "The supposedly contiguous Map isn't contiguous.");

  // Let's create a second Map.  It will have the same number of
  // global elements per process, but will distribute them
  // differently, in round-robin (1-D cyclic) fashion instead of
  // contiguously.
  RCP<const map_type> cyclicMap;
  {
    // We'll use the version of the Map constructor that takes, on
    // each MPI process, a list of the global elements in the Map
    // belonging to that process.  You can use this constructor to
    // construct an overlapping (also called "not 1-to-1") Map, in
    // which one or more elements are owned by multiple processes.  We
    // don't do that here; we make a nonoverlapping (also called
    // "1-to-1") Map.
    Array<global_ordinal_type>::size_type numEltsPerProc = 5;
    Array<global_ordinal_type> elementList (numEltsPerProc);

    const int numProcs = comm->getSize();
    const int myRank = comm->getRank();
    for (Array<global_ordinal_type>::size_type k = 0; k < numEltsPerProc; ++k)
      elementList[k] = myRank + k*numProcs;
    
    cyclicMap = rcp (new map_type (numGlobalElements, elementList, indexBase, 
				   comm, node));
  }

  // If there's more than one MPI process in the communicator,
  // then cyclicMap is definitely NOT contiguous.
  TEUCHOS_TEST_FOR_EXCEPTION(
    comm->getSize() > 1 && cyclicMap->isContiguous(),
    std::logic_error, 
    "The cyclic Map claims to be contiguous.");

  // contigMap and cyclicMap should always be compatible.  However, if
  // the communicator contains more than 1 process, then contigMap and
  // cyclicMap are NOT the same.
  TEUCHOS_TEST_FOR_EXCEPTION(! contigMap->isCompatible (*cyclicMap),
    std::logic_error,
    "contigMap should be compatible with cyclicMap, but it's not.");
  TEUCHOS_TEST_FOR_EXCEPTION(comm->getSize() > 1 && contigMap->isSameAs (*cyclicMap),
    std::logic_error,
    "contigMap should be compatible with cyclicMap, but it's not.");

  //////////////////////////////////////////////////////////////////////
  // We have maps now, so we can create vectors.
  //////////////////////////////////////////////////////////////////////

  // Since Tpetra::Vector takes four template parameters, its type
  // is long.  It's helpful to use a typedef.  You may also want to 
  // abbreviate the template parameters.  Standard abbreviations 
  // are "LO" for local_ordinal_type and "GO" for global_ordinal_type.
  typedef Tpetra::Vector<scalar_type, local_ordinal_type, 
    global_ordinal_type, node_type> vector_type;

  // Create a Vector with the contiguous Map.  This version of the
  // constructor will fill in the vector with zeros.
  RCP<vector_type> x = rcp (new vector_type (contigMap));

  // The copy constructor performs a deep copy.  
  // x and y have the same Map.
  RCP<vector_type> y = rcp (new vector_type (*x));

  // Create a Vector with the 1-D cyclic Map.  Calling the constructor
  // with false for the second argument leaves the data uninitialized,
  // so that you can fill it later without paying the cost of
  // initially filling it with zeros.
  RCP<vector_type> z = rcp (new vector_type (contigMap, false));

  // Set the entries of z to (pseudo)random numbers.  Please don't
  // consider this a good parallel pseudorandom number generator.
  z->randomize ();

  // Set the entries of x to all ones.
  //
  // The code below works because scalar_type=double, and C++
  // defines the implicit conversion from the integer 1 to double.
  // In general, you may use the commented-out line of code, if
  // the conversion from int to scalar_type is not defined for your
  // scalar type.
  x->putScalar (1);
  //x->putScalar (Teuchos::ScalarTraits<scalar_type>::one());

  // See comment above about type conversions to scalar_type.
  const scalar_type alpha = 3.14159;
  const scalar_type beta = 2.71828;
  const scalar_type gamma = -10;

  // x = beta*x + alpha*z
  //
  // This is a legal operation!  Even though the Maps of x and z are
  // not the same, their Maps are compatible.  Whether it makes sense
  // or not depends on your application.
  x->update (alpha, *z, beta);

  // See comment above about type conversions to scalar_type.
  y->putScalar (42);
  // y = gamma*y + alpha*x + beta*z
  y->update (alpha, *x, beta, *z, gamma);
  
  // Compute the 2-norm of y.  
  //
  // The norm may have a different type than scalar_type.  
  // For example, if scalar_type is complex, then the norm is real.  
  // The ScalarTraits "traits class" gives us the type of the norm.
  typedef Teuchos::ScalarTraits<scalar_type>::magnitudeType magnitude_type;
  const magnitude_type theNorm = y->norm2 ();
  
  // Print the norm of y on Proc 0.
  out << "Norm of y: " << theNorm << endl;
}

//
// The same main() driver routine as in the first Tpetra lesson.
//
int 
main (int argc, char *argv[]) 
{
  using std::endl;
  using Teuchos::RCP;

  Teuchos::oblackholestream blackHole;
  Teuchos::GlobalMPISession mpiSession (&argc, &argv, &blackHole);
  RCP<const Teuchos::Comm<int> > comm = 
    Tpetra::DefaultPlatform::getDefaultPlatform().getComm();

  const int myRank = comm->getRank();
  std::ostream& out = (myRank == 0) ? std::cout : blackHole;

  // We have a communicator and an output stream.
  // Let's do something with them!
  exampleRoutine (comm, out);

  return 0;
}
}}}

= Things not previously explained =

This lesson introduces three new topics: the "Node" template parameter of Tpetra objects, the `Teuchos::ScalarTraits` scalar traits class, and `Teuchos::Array`.  We will explain them both here.

== Tpetra's Node template parameter ==

The Node template parameter governs the way the Tpetra objects do parallelism within a node ("intranode," as opposed to MPI's internode parallelism).  We have implemented several different Node types.  All you need to know for now is that it is part of the type of the object.  You can't assign a matrix with one Node type to a matrix with a different Node type; they are incompatible.  Usually, you will use one Node type and Node instance for all the Tpetra objects that you create.

== The `Teuchos::ScalarTraits` scalar traits class ==

A _traits class_ maps from a C++ type to its attribute.  It is a standard C++ idiom for generic programming.  The C++ Standard Library comes with a few different traits classes, such as `std::numeric_traits`.  !ScalarTraits is a bit like `std::numeric_traits`, except that it is extensible.   That is, we can define new _specializations_ (definitions for new "input types").  For a given type `S`, `ScalarTraits<S>` can tell you the type of the magnitude of `S` (which is real if `S` is complex), how to compute the magnitude or extract the real or imaginary components, the definition of zero or one for `S`, and various other useful information.

== `Teuchos::Array` and other memory management classes ==

`Teuchos::Array` is an array container, templated on the type of objects that it contains.  It behaves much like `std::vector`.  The difference is that Array interoperates with the other Teuchos memory management classes.  For example, !ArrayView is a nonowning, nonpersistent view of part or all of an Array.  The `std::vector` class does not have nonowning views; passing std::vector by value copies the data, and there is no way to get a view of part of the `std::vector`.  Array and !ArrayView fix these deficiencies.  !ArrayRCP (not used here) is the array analog of !RCP; it allows shared ownership of an array.  For more details, please refer to the reference guide to the [http://www.cs.sandia.gov/~rabartl/TeuchosMemoryManagementSAND.pdf Teuchos Memory Management Classes].