#summary Tpetra Lesson 01: Initialization

= Lesson topics =

The Tpetra package provides next-generation distributed sparse linear algebra.  It includes sparse matrices, vectors, and other linear algebra objects, along with computational kernels. This lesson shows the initialization you need to do in order to start using Tpetra.  This differs slightly, depending on whether you are writing a code from scratch, or introducing Tpetra into an existing code base.  We will cover both cases with an example code.

= Copyright and license =

Please consider every code example in this lesson to begin with the following copyright and license information.
{{{
// @HEADER
// ***********************************************************************
//
//          Tpetra: Templated Linear Algebra Services Package
//                 Copyright (2008) Sandia Corporation
//
// Under the terms of Contract DE-AC04-94AL85000 with Sandia Corporation,
// the U.S. Government retains certain rights in this software.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// 1. Redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer.
//
// 2. Redistributions in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// 3. Neither the name of the Corporation nor the names of the
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY SANDIA CORPORATION "AS IS" AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL SANDIA CORPORATION OR THE
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//
// Questions? Contact Michael A. Heroux (maherou@sandia.gov)
//
// ************************************************************************
// @HEADER
}}}


= Initialization for a code written from scratch =

This section explains how to set up the distributed-memory parallel environment for using Tpetra, in a new code.  If you want to introduce Tpetra into an existing application, please see the next section.

Tpetra was written for distributed-memory parallel programming.  It uses  [http://en.wikipedia.org/wiki/Message_Passing_Interface MPI] (the Message Passing Interface) for this.  However, Tpetra will work correctly whether or not you have built Trilinos with MPI support.  It does so by interacting with MPI through an interface called `Teuchos::Comm`.  (If you are familiar with Epetra, this interface is analogous to `Epetra_Comm`.)  If MPI is enabled, then this wraps an MPI_Comm.  Otherwise, this is a "serial communicator" with one process, analogous to MPI_COMM_SELF.

Furthermore, Trilinos provides an MPI initialization interface, `Teuchos::GlobalMPISession`.  This calls MPI_Init and MPI_Finalize for you in an MPI build, and does not call them if you did not build Trilinos with MPI support.  The following code example shows how to initialize MPI (if available) and get a `Teuchos::Comm` corresponding to MPI_COMM_WORLD.

{{{
//
// Example of basic initialization boilerplate for using Tpetra.
//
// Includes MPI initialization, getting a Teuchos::Comm communicator,
// and printing out Tpetra version information.
//

#include <Tpetra_DefaultPlatform.hpp>
#include <Tpetra_Version.hpp>
#include <Teuchos_GlobalMPISession.hpp>
#include <Teuchos_oblackholestream.hpp>

void
exampleRoutine (const Teuchos::RCP<const Teuchos::Comm<int> >& comm,
		std::ostream& out)
{
  // Print out the Tpetra software version information.
  out << Tpetra::version() << std::endl << std::endl;
}

int 
main (int argc, char *argv[]) 
{
  // These "using" declarations make the code more concise, in that
  // you don't have to write the namespace along with the class or
  // object name.  This is especially helpful with commonly used
  // things like std::endl or Teuchos::RCP.
  using std::endl;
  using Teuchos::RCP;

  // A "black hole stream" prints nothing.  It's like /dev/null in
  // Unix-speak.  The typical MPI convention is that only MPI Rank 0
  // is allowed to print anything.  We enforce this convention by
  // setting Rank 0 to use std::cout for output, but all other ranks
  // to use the black hole stream.  It's more concise and less error
  // prone than having to check the rank every time you want to print.
  Teuchos::oblackholestream blackHole;

  // Start up MPI, if using MPI.  Trilinos doesn't have to be built
  // with MPI; it's called a "serial" build if you build without MPI.
  // GlobalMPISession hides this implementation detail.
  //
  // Note the third argument.  If you pass GlobalMPISession the
  // address of an std::ostream, it will print a one-line status
  // message with the rank on each MPI process.  This may be
  // undesirable if running with a large number of MPI processes.
  // You can avoid printing anything here by passing in either 
  // NULL or the address of a Teuchos::oblackholestream.
  Teuchos::GlobalMPISession mpiSession (&argc, &argv, NULL);

  // Get a pointer to the communicator object representing
  // MPI_COMM_WORLD.  getDefaultPlatform.getComm() doesn't create a
  // new object every time you call it; it just returns the same
  // communicator each time.  Thus, you can call it anywhere and get
  // the same communicator.  (This is handy if you don't want to pass
  // a communicator around everywhere, though it's always better to
  // parameterize your algorithms on the communicator.)
  //
  // "Tpetra::DefaultPlatform" knows whether or not we built with MPI
  // support.  If we didn't build with MPI, we'll get a "communicator"
  // with size 1, whose only process has rank 0.
  RCP<const Teuchos::Comm<int> > comm = 
    Tpetra::DefaultPlatform::getDefaultPlatform().getComm();

  const int myRank = comm->getRank();
  const int numProcs = comm->getSize();

  // The stream to which to write output.  Only MPI Rank 0 gets to
  // write to stdout; the other MPI processes get a "black hole
  // stream" (see above).
  std::ostream& out = (myRank == 0) ? std::cout : blackHole;

  // We have a communicator and an output stream.
  // Let's do something with them!
  exampleRoutine (comm, out);

  // GlobalMPISession calls MPI_Finalize() in its destructor, if
  // appropriate.  You don't have to do anything here!  Just return
  // from main().  Isn't that helpful?
  return 0;
}
}}}

= Initialization for an existing code =

{{{
//
// Example of basic initialization boilerplate for using Tpetra.
//
// Includes MPI initialization, getting a Teuchos::Comm communicator,
// and printing out Tpetra version information.
//

#include <Tpetra_DefaultPlatform.hpp>
#include <Tpetra_Version.hpp>
#include <Teuchos_GlobalMPISession.hpp>
#include <Teuchos_oblackholestream.hpp>

void
exampleRoutine (const Teuchos::RCP<const Teuchos::Comm<int> >& comm,
		std::ostream& out)
{
  // Print out the Tpetra software version information.
  out << Tpetra::version() << std::endl << std::endl;
}

int 
main (int argc, char *argv[]) 
{
  // These "using" declarations make the code more concise, in that
  // you don't have to write the namespace along with the class or
  // object name.  This is especially helpful with commonly used
  // things like std::endl or Teuchos::RCP.
  using std::endl;
  using Teuchos::Comm;
  using Teuchos;:MpiComm;
  using Teuchos::opaqueWrapper;
  using Teuchos::RCP;

  // ... Your code here, that calls MPI_Init and gets an MPI_Comm ...
  MPI_Comm yourComm = ...;

  // Get a pointer to the communicator object that encapsulates 
  // your communicator.  In a very, very recent version of Trilinos,
  // you may use the first line (commented out) instead of the 
  // second line below.  Both of these lines assume that you are
  // responsible for calling MPI_Comm_free on your MPI_Comm,
  // if necessary (it's not necessary for MPI_COMM_WORLD).
  //
  //RCP<const Comm<int> > comm = rcp (new MpiComm<int> (yourComm));
  RCP<const Comm<int> > comm = rcp (new MpiComm<int> (opaqueWrapper<MPI_Comm> (yourComm)));
  
  const int myRank = comm->getRank();
  const int numProcs = comm->getSize();

  // The stream to which to write output.  Only MPI Rank 0 gets to
  // write to stdout; the other MPI processes get a "black hole
  // stream" (see above).
  Teuchos::oblackholestream blackHole;
  std::ostream& out = (myRank == 0) ? std::cout : blackHole;

  // We have a communicator and an output stream.
  // Let's do something with them!
  exampleRoutine (comm, out);

  // Since you called MPI_Init, you are responsible for calling MPI_Finalize.
  (void) MPI_Finalize ();
  return 0;
}
}}}