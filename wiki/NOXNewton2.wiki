{{{
//
// ComputeF: computes F(x) for a given Epetra_Vector x
//
// UpdateJacobian: updates the entries of the Jacobian matrix.  
//
// The Jacobian matrix in this case can be written as
//
//     J = L + diag(lambda*exp(x[i])),
//
// where L corresponds to the discretization of a Laplacian, and diag
// is a diagonal matrix with entries lambda*exp(x[i]).  Thus, to
// update the jacobian we simply update the diagonal entries, by
// supplying x as a vector.  Similarly, to compute F(x), we reset J to
// be equal to L, multiply it by the (distributed) vector x, and then
// add the diagonal contribution.
// ==========================================================================
class PDEProblem {
public:
  //
  // Constructor.
  //
  // Input arguments:
  //
  // nx: Number of (internal) elements along the x axis.
  // ny: Number of (internal) elements along the y axis.
  // lambda: Scaling parameter of the Jacobian matrix.
  // Comm: Communicator over which to distribute the matrix.
  //
  PDEProblem (const int nx, 
	      const int ny, 
	      const double lambda,
	      const Epetra_Comm& Comm) :
    nx_(nx), ny_(ny), lambda_(lambda) 
  {

    hx_ = 1.0/(nx_-1);
    hy_ = 1.0/(ny_-1);
    Matrix_ = CreateLaplacian (nx_,ny_, Comm);
  }

  // The destructor doesn't need to do anything.  RCPs are smart
  // pointers; they handle deallocation automatically.
  ~PDEProblem() {}

  // Compute F(x).
  void 
  ComputeF (const Epetra_Vector& x, Epetra_Vector& f) 
  {
    // Reset the diagonal entries.
    double diag = 2.0/(hx_*hx_) + 2.0/(hy_*hy_);

    int NumMyElements = Matrix_->Map().NumMyElements();

    // Get the list of the global elements that belong to my MPI process.
    int* MyGlobalElements = Matrix_->Map ().MyGlobalElements ();

    for (int i = 0; i < NumMyElements; ++i) {
      // Update the diagonal entry of the matrix.
      Matrix_->ReplaceGlobalValues (MyGlobalElements[i], 1, &diag, MyGlobalElements+i);
    }
    // Sparse matrix-vector product.  
    // Interprocess communication happens here.
    Matrix_->Multiply (false, x, f);

    for (int i = 0; i < NumMyElements; ++i) {
      // Include the contribution from the current diagonal entry.
      f[i] += lambda_*exp(x[i]);
    }
  }

  // Update the Jacobian matrix for a given vector x (see class
  // documentation for an explanation of how x contributes to the
  // update formula).
  void 
  UpdateJacobian (const Epetra_Vector & x) 
  {
    double diag =  2.0/(hx_*hx_) + 2.0/(hy_*hy_);

    int NumMyElements = Matrix_->Map ().NumMyElements ();

    // Get the list of the global elements that belong to my MPI process.
    int* MyGlobalElements = Matrix_->Map ().MyGlobalElements ();

    for (int i = 0; i < NumMyElements; ++i) {
      // Update the current diagonal entry.
      double newdiag = diag + lambda_*exp(x[i]);
      Matrix_->ReplaceGlobalValues (MyGlobalElements[i], 1, 
				    &newdiag, MyGlobalElements+i);
    }

  }

  // Return a pointer to the internally stored matrix.
  Teuchos::RCP<Epetra_CrsMatrix> GetMatrix() {
    return Matrix_;
  }

private:
  int nx_, ny_;
  double hx_, hy_;
  Teuchos::RCP<Epetra_CrsMatrix> Matrix_;
  double lambda_;
};

// ==========================================================================
// SimpleProblemInterface defines the interface between NOX and our
// nonlinear problem to solve.  Its constructor accepts a PDEProblem
// object, which it uses to update the Jacobian and compute F(x).
// This interface is a bit crude; for example, it does not provide a
// PrecMatrix or Preconditioner.
// ==========================================================================
class SimpleProblemInterface : 
  public NOX::Epetra::Interface::Required,
  public NOX::Epetra::Interface::Jacobian
{
public:

  // The constructor takes a PDEProblem pointer.
  SimpleProblemInterface (Teuchos::RCP<PDEProblem> Problem) :
    Problem_(Problem) 
  {}

  // The destructor doesn't need to do anything, because RCPs are
  // smart pointers; they handle deallocation automatically.
  ~SimpleProblemInterface() {}

  bool 
  computeF (const Epetra_Vector& x, 
	    Epetra_Vector& f,
	    NOX::Epetra::Interface::Required::FillType F)
  {
    Problem_->ComputeF (x, f);
    return true;
  }

  bool 
  computeJacobian (const Epetra_Vector& x, Epetra_Operator& Jac)
  {
    Problem_->UpdateJacobian (x);
    return true;
  }

  bool 
  computePrecMatrix (const Epetra_Vector& x, Epetra_RowMatrix& M) 
  {
    throw std::runtime_error ("*** SimpleProblemInterface does not implement "
			      "computing an explicit preconditioner from an "
			      "Epetra_RowMatrix ***");
  }  

  bool 
  computePreconditioner(const Epetra_Vector & x, Epetra_Operator & O)
  {
    throw std::runtime_error ("*** SimpleProblemInterface does not implement "
			      "computing an explicit preconditioner from an "
			      "Epetra_Operator ***");
  }  

private:
  Teuchos::RCP<PDEProblem> Problem_;
};

//
// Test driver routine.
//
int 
main (int argc, char **argv)
{
  using Teuchos::ParameterList;
  using Teuchos::parameterList;
  using Teuchos::RCP;
  using Teuchos::rcp;

#ifdef HAVE_MPI
  MPI_Init(&argc, &argv);
  Epetra_MpiComm Comm(MPI_COMM_WORLD);
#else
  Epetra_SerialComm Comm;
#endif

  // Parameters for setting up the nonlinear PDE.  The 2-D regular
  // mesh on which the PDE's discretization is defined is nx by ny
  // (internal nodes; we assume Dirichlet boundary conditions have
  // been condensed out).
  const int nx = 5;
  const int ny = 6;  
  const double lambda = 1.0;

  RCP<PDEProblem> Problem = rcp (new PDEProblem (nx, ny, lambda, Comm));

  // Prepare the initial guess vector.  It should be a vector in the
  // domain of the nonlinear problem's matrix.
  Epetra_Vector InitialGuess (Problem->GetMatrix ()->OperatorDomainMap ());

  // Make the starting solution a zero vector.
  InitialGuess.PutScalar(0.0);

  // Set up the problem interface.
  RCP<SimpleProblemInterface> interface (new SimpleProbleInterface (Problem));

  // Create the top-level parameter list to control NOX.
  RCP<ParameterList> params = parameterList ("NOX");

  // Tell the nonlinear solver to use line search.
  params->set ("Nonlinear Solver", "Line Search Based");

  //
  // Set the printing parameters in the "Printing" sublist.
  //
  ParameterList& printParams = params->sublist ("Printing");
  printParams.set ("MyPID", Comm.MyPID ()); 
  printParams.set ("Output Precision", 3);
  printParams.set ("Output Processor", 0);
  printParams.set ("Output Information", 
		   NOX::Utils::OuterIteration + 
		   NOX::Utils::OuterIterationStatusTest + 
		   NOX::Utils::InnerIteration +
		   NOX::Utils::Parameters + 
		   NOX::Utils::Details + 
		   NOX::Utils::Warning);

  //
  // Set the nonlinear solver parameters.
  //

  // Linear search parameters.
  Teuchos::ParameterList& searchParams = params->sublist ("Line Search");
  searchParams.set ("Method", "More'-Thuente");

  // Parameters for picking the search direction.
  Teuchos::ParameterList& dirParams = params->sublist ("Direction");
  // Use Newton's method to pick the search direction.
  dirParams.set ("Method", "Newton");

  // Parameters for Newton's method.
  Teuchos::ParameterList& newtonParams = dirParams.sublist("Newton");
  newtonParams.set("Forcing Term Method", "Constant");
  
  //
  // Set linear solver parameters for Newton's method.
  //
  Teuchos::ParameterList& lsParams = newtonParams.sublist("Linear Solver");
  // Use Aztec's implementation of GMRES, with at most 800 iterations,
  // a residual tolerance of 1.0e-4, with output every 50 iterations,
  // and Aztec's native ILU preconditioner.
  lsParams.set ("Aztec Solver", "GMRES");  
  lsParams.set ("Max Iterations", 800);  
  lsParams.set ("Tolerance", 1e-4);
  lsParams.set ("Output Frequency", 50);    
  lsParams.set ("Aztec Preconditioner", "ilu"); 

  RCP<Epetra_CrsMatrix> A = Problem.GetMatrix();

  RCP<NOX::Epetra::Interface::Required> iReq = interface;
  RCP<NOX::Epetra::Interface::Jacobian> iJac = interface;

  RCP<NOX::Epetra::LinearSystemAztecOO> linSys = 
    rcp (new NOX::Epetra::LinearSystemAztecOO (printParams, lsParams,
					       iReq, iJac, A, InitialGuess));

  // Need a NOX::Epetra::Vector for constructor
  NOX::Epetra::Vector noxInitGuess (InitialGuess, NOX::DeepCopy);
  RCP<NOX::Epetra::Group> group = 
    rcp (new NOX::Epetra::Group (printParams, iReq, noxInitGuess, linSys));

  //
  // Set up NOX's iteration stopping criteria ("status tests").
  //

  // ||F(X)||_2 / N < 1.0e-4, where N is the length of F(X).
  //
  // NormF has many options for setting up absolute vs. relative
  // (scaled by the norm of the initial guess) tolerances, scaling or
  // not scaling by the length of F(X), and choosing a different norm
  // (we use the 2-norm here).
  RCP<NOX::StatusTest::NormF> testNormF = 
    rcp (new NOX::StatusTest::NormF (1.0e-4));

  // At most 20 (nonlinear) iterations.
  RCP<NOX::StatusTest::MaxIters> testMaxIters = 
    rcp (new NOX::StatusTest::MaxIters (20));

  // Combine the above two stopping criteria (normwise convergence,
  // and maximum number of nonlinear iterations).  The result tells
  // NOX to stop if at least one of them is satisfied.
  RCP<NOX::StatusTest::Combo> combo = 
    rcp (new NOX::StatusTest::Combo (NOX::StatusTest::Combo::OR, 
				     testNormF, testMaxIters));

  // Create the NOX nonlinear solver.
  RCP<NOX::Solver::Generic> solver = 
    NOX::Solver::buildSolver (group, combo, params);

  // Solve the nonlinear system.
  NOX::StatusTest::StatusType status = solver->solve();

  // Print the result.
  if (Comm.MyPID() == 0) {
    cout << endl << "-- Parameter List From Solver --" << endl;
    solver->getList ().print (cout);
  }

  // Get the Epetra_Vector with the final solution from the solver.
  const NOX::Epetra::Group& finalGroup = 
    dynamic_cast<const NOX::Epetra::Group&>(solver->getSolutionGroup());

  const Epetra_Vector& finalSolution = 
    (dynamic_cast<const NOX::Epetra::Vector&>(finalGroup.getX ())).getEpetraVector ();

  if (Comm.MyPID() == 0) {
    cout << "Computed solution : " << endl;
  }
  cout << finalSolution;

#ifdef HAVE_MPI
  MPI_Finalize();
#endif
  return EXIT_SUCCESS;
} /* main */
}}}